{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7f976a49d1eb4c39a7166fd875a7996c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_282e127fd8e04816abd97161d2d21c8b","IPY_MODEL_67f5978eb4ed45e8a78cc7f41871d514","IPY_MODEL_b64d320743084cc88d29396b2f6e1ae2"],"layout":"IPY_MODEL_3f42716c528447d7b952bcc3fcf78db8"}},"282e127fd8e04816abd97161d2d21c8b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_823030528d784084acb2406e1316607d","placeholder":"​","style":"IPY_MODEL_2fb1b36a4a8b4a0386f89607aa0d3bed","value":"Computing transition probabilities: 100%"}},"67f5978eb4ed45e8a78cc7f41871d514":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eeed9c866e9d42e4be13824600b99459","max":1977,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e09fd092954442da86f8ae3233e21905","value":1977}},"b64d320743084cc88d29396b2f6e1ae2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85a173cd84d74329ac692c4843a4960b","placeholder":"​","style":"IPY_MODEL_a954d5d761604385b6706332e8e2c534","value":" 1977/1977 [00:37&lt;00:00, 74.41it/s]"}},"3f42716c528447d7b952bcc3fcf78db8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"823030528d784084acb2406e1316607d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fb1b36a4a8b4a0386f89607aa0d3bed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eeed9c866e9d42e4be13824600b99459":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e09fd092954442da86f8ae3233e21905":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"85a173cd84d74329ac692c4843a4960b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a954d5d761604385b6706332e8e2c534":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Analysis of Facebook Large Page-Page Dataset using Mutual Consumer Behavioral page liking & Community Detection based upon Graph/SubGraph Structure \n"],"metadata":{"id":"MuTf21tZnePS"}},{"cell_type":"markdown","source":["**Team details**\n","- Kruthika Suresh &emsp;&emsp;&emsp; PES1UG19CS233\n","- Yousha Mahamuni &emsp;&ensp;&nbsp; PES2UG19CS468\n","- Shrikar Madhu &emsp;&emsp;&emsp;&ensp; PES1UG19CS470\n","- Smriti Tilak &emsp;&emsp;&emsp;&emsp;&emsp; PES1UG19CS486"],"metadata":{"id":"Ma_CIHWLnmI9"}},{"cell_type":"code","source":["!pip install node2vec"],"metadata":{"id":"ivTLPPsdMuKv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kp7-oLjsnNu1"},"outputs":[],"source":["import pandas as pd\n","import networkx as nx\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.stats as sp\n","import collections\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, matthews_corrcoef, confusion_matrix, classification_report\n","from node2vec import Node2Vec as n2v"]},{"cell_type":"markdown","source":["#Dataset Description\n","\n","> This webgraph is a page-page graph of verified Facebook sites. Nodes represent official Facebook pages while the links are mutual likes between sites. Node features are extracted from the site descriptions that the page owners created to summarize the purpose of the site. This graph was collected through the Facebook Graph API in November 2017 and restricted to pages from 4 categories which are defined by Facebook. These categories are: politicians, governmental organizations, television shows and companies. \n","\n","> The links are formed based on mutual likes between pages i.e., an edge is present between nodes if a user has liked both nodes\n","\n","The dataset has been taken from SNAP  \n","> Link: [Facebook Large Page-Page Network](https://snap.stanford.edu/data/facebook-large-page-page-network.html)"],"metadata":{"id":"xz-UfP5Ry0O-"}},{"cell_type":"markdown","source":["Gephi Visualisations can be viewed in the slides: [Link](https://docs.google.com/presentation/d/1kGk5RWkH5xxmSVGHWpztKQgYqHe9uSs4/edit?usp=sharing&ouid=115873317750525938978&rtpof=true&sd=true)"],"metadata":{"id":"88vOYk9SnuBJ"}},{"cell_type":"code","source":["edges = pd.read_csv(\"musae_facebook_edges.csv\")\n","target = pd.read_csv(\"musae_facebook_target.csv\")"],"metadata":{"id":"_JInlOuMnlGS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["edges.head()"],"metadata":{"id":"fipdppGUnxpp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target.head()"],"metadata":{"id":"LVz9FOfZqR5c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target['page_type'].hist(color = \"purple\")"],"metadata":{"id":"P0_s72v5r0l0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Not all classes have the same number of the nodes. TV Show has the least nodes and Government pages are the highest in the dataset"],"metadata":{"id":"Nw0ZwPquttFw"}},{"cell_type":"code","source":["print(target.groupby(['page_type']).count())"],"metadata":{"id":"MQ-YVGus7PxP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Creating the Graph"],"metadata":{"id":"CyRGn-GFucRw"}},{"cell_type":"code","source":["G = nx.Graph()\n","\n","#Adding nodes with attribute = page_type\n","for i, j in zip(target['id'], target['page_type']):\n","  G.add_node(i, page_type = j)\n","\n","#Adding edges between the nodes\n","for i, j in zip(edges['id_1'], edges['id_2']):\n","  G.add_edge(i, j)"],"metadata":{"id":"I0usBwpqtY1I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Graph Statistics and Visualisation"],"metadata":{"id":"aVoDKLvX0Sjl"}},{"cell_type":"code","source":["print(\"Number of nodes in the graph: \", G.number_of_nodes())\n","print(\"Number of edges in the graph: \", G.number_of_edges())"],"metadata":{"id":"tGMbjQ0CvTm4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nx.is_directed(G)"],"metadata":{"id":"-7ONXI2jv9DW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The graph we have is undirected"],"metadata":{"id":"Sq5HlDkX9vp-"}},{"cell_type":"code","source":["print(\"Number of self loops in the graph: \", nx.number_of_selfloops(G))\n","print(\"Number of nodes with self loops: \", len(list(nx.nodes_with_selfloops(G))))\n","print(\"Density of the graph: \", nx.density(G))\n","print(\"Transitivity of the graph: \", nx.transitivity(G))"],"metadata":{"id":"mtkhWCggD7mu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A page does not have more than one link to itself as the number of self loops is equal to the number of nodes with self loops. The graph is highly sparse."],"metadata":{"id":"taF9h6SZKoeP"}},{"cell_type":"code","source":["nx.attribute_assortativity_coefficient(G, \"page_type\")"],"metadata":{"id":"pSkll23LIcdj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The Assortativity is very high which could mean people who liked pages of a certain type mostly tend to like similar pages"],"metadata":{"id":"SSu4sOXYNp9T"}},{"cell_type":"code","source":["'''plt.figure(figsize=(100, 100))\n","nx.draw(G, with_labels = True)'''"],"metadata":{"id":"g1a9laWCwI1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since the above command takes approx 30 mins to execute, we have linked the output below.\n","\n","[Link to the network graph](https://drive.google.com/file/d/15XdBDt-ZQodME5D4DbXS2qaaNkP8akGy/view?usp=sharing)"],"metadata":{"id":"KVwk_3hiBzvh"}},{"cell_type":"code","source":["degree_sequence = sorted([deg for node, deg in G.degree()], reverse=True)\n","degree_count = collections.Counter(degree_sequence)\n","deg, count = zip(*degree_count.items())\n","\n","fig, ax = plt.subplots(figsize=(20, 10))\n","plt.bar(deg, count, width=0.80, color=\"purple\")\n","plt.title(\"Degree Distribution for the Graph\")\n","plt.ylabel(\"Count\")\n","plt.xlabel(\"Degree\")\n","plt.xlim(0, 200)\n","plt.show()"],"metadata":{"id":"pv2iiSYfwyxU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The degree distribution follows power-law, hence, it is scale-free."],"metadata":{"id":"tlyg15kmN2-T"}},{"cell_type":"code","source":["d = dict(G.degree())\n","isolates = [k for k,v in d.items() if float(v) == 0]\n","print(\"Number of nodes with degree zero: \", len(isolates))"],"metadata":{"id":"gSaorh5T-Z11"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Sampling the Graph"],"metadata":{"id":"II9I_nSJ7bVz"}},{"cell_type":"markdown","source":["> The graph has over 20000 nodes and over 100000 edges and performing analysis on it is computationally expensive. This is why we chose to subsample the graph. We have decided to drop nodes that have a degree < 40. It effectively brought down the node count to 1977 which was manageable."],"metadata":{"id":"B5C2qlU1OBIP"}},{"cell_type":"code","source":["#Get all nodes with degree < 40\n","\n","deg = [k for k,v in d.items() if float(v) < 40]\n","len(deg)"],"metadata":{"id":"JXsONGRP_5ER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Before removing nodes: \")\n","print(\"Number of nodes in the graph: \", G.number_of_nodes())\n","print(\"Number of edges in the graph: \", G.number_of_edges())\n","\n","G.remove_nodes_from(deg)\n","print()\n","print(\"After removing nodes: \")\n","print(\"Number of nodes in the graph: \", G.number_of_nodes())\n","print(\"Number of edges in the graph: \", G.number_of_edges())"],"metadata":{"id":"LxfyT_dyFkJ1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We have removed the nodes with a degree less than 40 in order to reduce graph size"],"metadata":{"id":"pVvRW1aO7mJH"}},{"cell_type":"markdown","source":["### Sampled Graph Statistics"],"metadata":{"id":"KydoJCsn75Pz"}},{"cell_type":"code","source":["print(\"Density of the graph: \", nx.density(G))\n","print(\"Assortativity: \", nx.attribute_assortativity_coefficient(G, \"page_type\"))\n","print(\"Transitivity of the graph: \", nx.transitivity(G))\n","print(\"Number of self loops in the graph: \", nx.number_of_selfloops(G))\n","print(\"Number of nodes with self loops: \", len(list(nx.nodes_with_selfloops(G))))"],"metadata":{"id":"jTtBPhEm79yY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The new graph has a higher density which helps with the computation. Assortativity approximately remains the same"],"metadata":{"id":"d0lFV-0UOol5"}},{"cell_type":"code","source":["degree_sequence = sorted([deg for node, deg in G.degree()], reverse=True)\n","degree_count = collections.Counter(degree_sequence)\n","deg, count = zip(*degree_count.items())\n","\n","fig, ax = plt.subplots(figsize=(20, 10))\n","plt.bar(deg, count, width=0.80, color=\"purple\")\n","plt.title(\"Degree Distribution for the Sampled Graph\")\n","plt.ylabel(\"Count\")\n","plt.xlabel(\"Degree\")\n","plt.xlim(0, 200)\n","plt.show()"],"metadata":{"id":"oy3RplDq8XJC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The distribution is still skewed but not as heavily skewed as in the case of the power law distribution observed earlier"],"metadata":{"id":"-7vF5R7XB097"}},{"cell_type":"markdown","source":["### Plotting the Subgraphs for the 4 classes"],"metadata":{"id":"zxjKWH8iCBNp"}},{"cell_type":"code","source":["attr = dict(nx.get_node_attributes(G, \"page_type\"))\n","\n","govt_nodes = [k for k, v in attr.items() if v == \"government\"]\n","tv_nodes = [k for k, v in attr.items() if v == \"tvshow\"]\n","company_nodes = [k for k, v in attr.items() if v == \"company\"]\n","politician_nodes = [k for k, v in attr.items() if v == \"politician\"]"],"metadata":{"id":"vTHWFjxw8cx8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Government Subgraph"],"metadata":{"id":"U4gmjaqLRPt8"}},{"cell_type":"code","source":["G_govt = G.subgraph(govt_nodes)\n","plt.figure(figsize=(100, 100))\n","nx.draw(G_govt, with_labels = True)"],"metadata":{"id":"IwT_HG3O8fNn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nx.info(G_govt)"],"metadata":{"id":"fDfqTovm8igj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The government class retained the most number of nodes as compared to the other sub-groups. This would mean that the government pages are the most mutually liked. We can also observe that most nodes are clustered together this could be due to the fact that people interested in a particular diplomatic opinion tend to follow/like similar pages with support their opinions only."],"metadata":{"id":"vG9BqVQdPEAc"}},{"cell_type":"markdown","source":["#### TV Shows Subgraph"],"metadata":{"id":"4hZLAI3dRT2G"}},{"cell_type":"code","source":["G_tv = G.subgraph(tv_nodes)\n","plt.figure(figsize=(100, 100))\n","nx.draw(G_tv, with_labels = True)"],"metadata":{"id":"9oqFlFZa8l6F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nx.info(G_tv)"],"metadata":{"id":"MbgNtAbY8mmI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The TV Shows class only retained 117 nodes after subsampling. A few dense clusters can be observed in the graph. This could be because people tend to follow shows of a similar genre or the popular shows could be clustered together"],"metadata":{"id":"CNdkScHRRYWr"}},{"cell_type":"markdown","source":["#### Company Subgraph"],"metadata":{"id":"tBnqdMuzUMdi"}},{"cell_type":"code","source":["G_company = G.subgraph(company_nodes)\n","plt.figure(figsize=(100, 100))\n","nx.draw(G_company, with_labels = True)"],"metadata":{"id":"4LI5UAQ98rAg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The company subgraph"],"metadata":{"id":"SxqbcObISHYh"}},{"cell_type":"code","source":["nx.info(G_company)"],"metadata":{"id":"BbQRMQEt8tyc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Politician Subgraph"],"metadata":{"id":"wCXljO8rURWp"}},{"cell_type":"code","source":["G_politician = G.subgraph(politician_nodes)\n","plt.figure(figsize=(100, 100))\n","nx.draw(G_politician, with_labels = True)"],"metadata":{"id":"v_xDSmlu8wwD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nx.info(G_politician)"],"metadata":{"id":"Hwmt8nfo85lC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Community Detection in Subgraphs"],"metadata":{"id":"fEVc454A8go6"}},{"cell_type":"code","source":["!pip uninstall community\n","!pip install python-louvain\n","\n","import matplotlib.colors as mpcol\n","import matplotlib.cm as cm"],"metadata":{"id":"lRBYdJU6MZ61"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","from community import community_louvain\n","\n","def find_communities(G):\n","    start_time = time.time()\n","    partition = community_louvain.best_partition(G)\n","    part_dict = {}\n","    values = []\n","    for node in G.nodes():\n","        values.append(partition.get(node))\n","        part_dict.update({node:partition.get(node)})\n","    communities_louvain= max(values)+1\n","    end_time = time.time()\n","    mod_louvain = community_louvain.modularity(partition, G)\n","    print('Communities found using the Louvain algorithm: {} \\nModularity: {} \\nTime for finding the communities: {} s'.format(communities_louvain, mod_louvain,round((end_time-start_time),3)))\n","    return part_dict"],"metadata":{"id":"DBod_BKAOoS6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Communities in the Government Subgraph"],"metadata":{"id":"zsleiTo5aNSY"}},{"cell_type":"code","source":["comm_govt = find_communities(G_govt)"],"metadata":{"id":"kdtbNbi5XDvJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Set the node attributes ussing the partition dictionary.\n","nx.set_node_attributes(G_govt, comm_govt, 'community')\n","\n","# Get a list of len(nodes) with their corresponding community\n","communities = [(G_govt.nodes()[i]['community']) for i in G_govt.nodes()]"],"metadata":{"id":"LhPkU3wxXF0f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["colours = ['#E1CF3F','#F47757','#FD4581','#97577C','#BDA7A9','#E1CF3F','#F47757','#FD4581',\n","                 '#e44623','#e45a6a','#c9d3e6','#7d513d',\n","                 '#e65949','#d6b240','#382a29','#d8d4c9',\n","                 '#e4cc34','#ccb42c','#bc8ca4','#3c84c4',\n","                 '#dd4d3d','#52172f','#63494a','#e2d5d3',\n","                 '#f7abcc','#e085a1','#943d39','#2d1d19']\n","\n","Barragan = mpcol.ListedColormap(colours, name='Barragan')"],"metadata":{"id":"I46cgDEcXHrG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["com = [x[1] for x in G_govt.nodes(data='community')]\n","norm = mpcol.Normalize(vmin=min(com), vmax=max(com), clip=True)\n","mapper = cm.ScalarMappable(norm=norm, cmap=Barragan)\n","nc=[mapper.to_rgba(x) for x in com]"],"metadata":{"id":"K7QkQNXQXJdJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(100, 100))\n","nx.draw(G_govt, node_color = nc, with_labels = False)"],"metadata":{"id":"KcPelX-TXL5b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["govt_commlist = dict(nx.get_node_attributes(G_govt, \"community\"))\n","govt_commlist1 = {}\n","for k, v in govt_commlist.items():\n","  if v not in govt_commlist1:\n","    govt_commlist1[v] = [k]\n","  else:\n","    govt_commlist1[v].append(k)\n","print(govt_commlist1)"],"metadata":{"id":"UaEXp8FMw2AJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_govt = {}\n","for k in govt_commlist1:\n","  final_govt[k] = []\n","  for v in govt_commlist1[k]:\n","    r = target['page_name'][v]\n","    final_govt[k].append(r)\n","print(final_govt)"],"metadata":{"id":"5c6vNU3F9crs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Communities in the TV Shows Subgraph"],"metadata":{"id":"QnAbqLNKaTMz"}},{"cell_type":"code","source":["comm_tv = find_communities(G_tv)"],"metadata":{"id":"AMIR7U-nXOV0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Set the node attributes ussing the partition dictionary.\n","nx.set_node_attributes(G_tv, comm_tv, 'community')\n","\n","# Get a list of len(nodes) with their corresponding community\n","communities = [(G_tv.nodes()[i]['community']) for i in G_tv.nodes()]"],"metadata":{"id":"jMqjKSMGXPJf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["com = [x[1] for x in G_tv.nodes(data='community')]\n","norm = mpcol.Normalize(vmin=min(com), vmax=max(com), clip=True)\n","mapper = cm.ScalarMappable(norm=norm, cmap=Barragan)\n","nc=[mapper.to_rgba(x) for x in com]"],"metadata":{"id":"vaRO6yhjXRxY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(100, 100))\n","nx.draw(G_tv, node_color = nc, with_labels = False)"],"metadata":{"id":"spA77v1gXS59"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tv_commlist = dict(nx.get_node_attributes(G_tv, \"community\"))\n","tv_commlist1 = {}\n","for k, v in tv_commlist.items():\n","  if v not in tv_commlist1:\n","    tv_commlist1[v] = [k]\n","  else:\n","    tv_commlist1[v].append(k)\n","print(tv_commlist1)"],"metadata":{"id":"cwa6-Nl-xOeK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_tv = {}\n","for k in tv_commlist1:\n","  final_tv[k] = []\n","  for v in tv_commlist1[k]:\n","    r = target['page_name'][v]\n","    final_tv[k].append(r)\n","print(final_tv)"],"metadata":{"id":"ejlw9m6J9EoO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Communities in the Company Subgraph"],"metadata":{"id":"rl6tGr4LaYdt"}},{"cell_type":"code","source":["comm_company = find_communities(G_company)"],"metadata":{"id":"2-Lr1KOAXSte"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Set the node attributes ussing the partition dictionary.\n","nx.set_node_attributes(G_company, comm_company, 'community')\n","\n","# Get a list of len(nodes) with their corresponding community\n","communities = [(G_company.nodes()[i]['community']) for i in G_company.nodes()]"],"metadata":{"id":"zOyVuQezXXbl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["com = [x[1] for x in G_company.nodes(data='community')]\n","norm = mpcol.Normalize(vmin=min(com), vmax=max(com), clip=True)\n","mapper = cm.ScalarMappable(norm=norm, cmap=Barragan)\n","nc=[mapper.to_rgba(x) for x in com]"],"metadata":{"id":"UnWFuyCWXXPu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(100, 100))\n","nx.draw(G_company, node_color = nc, with_labels = False)"],"metadata":{"id":"DID6mib6XaSO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comp_commlist = dict(nx.get_node_attributes(G_company, \"community\"))\n","comp_commlist1 = {}\n","for k, v in comp_commlist.items():\n","  if v not in comp_commlist1:\n","    comp_commlist1[v] = [k]\n","  else:\n","    comp_commlist1[v].append(k)\n","print(comp_commlist1)"],"metadata":{"id":"ttWagyqXxan5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_comp = {}\n","for k in comp_commlist1:\n","  final_comp[k] = []\n","  for v in comp_commlist1[k]:\n","    r = target['page_name'][v]\n","    final_comp[k].append(r)\n","print(final_comp)"],"metadata":{"id":"Q2OR4xgA8yVp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Communities in the Politician Subgraph"],"metadata":{"id":"rYe7abp3adrc"}},{"cell_type":"code","source":["comm_politician = find_communities(G_politician)"],"metadata":{"id":"dWdhbRh_Xdk-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Set the node attributes ussing the partition dictionary.\n","nx.set_node_attributes(G_politician, comm_politician, 'community')\n","\n","# Get a list of len(nodes) with their corresponding community\n","communities = [(G_politician.nodes()[i]['community']) for i in G_politician.nodes()]"],"metadata":{"id":"hl0DoYraXdi-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["com = [x[1] for x in G_politician.nodes(data='community')]\n","norm = mpcol.Normalize(vmin=min(com), vmax=max(com), clip=True)\n","mapper = cm.ScalarMappable(norm=norm, cmap=Barragan)\n","nc=[mapper.to_rgba(x) for x in com]"],"metadata":{"id":"W5uEbNzxXdWc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(100, 100))\n","nx.draw(G_politician, node_color = nc, with_labels = False)"],"metadata":{"id":"z0wvWWLxXiRC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pol_commlist = dict(nx.get_node_attributes(G_politician, \"community\"))\n","pol_commlist1 = {}\n","for k, v in pol_commlist.items():\n","  if v not in pol_commlist1:\n","    pol_commlist1[v] = [k]\n","  else:\n","    pol_commlist1[v].append(k)\n","print(pol_commlist1)"],"metadata":{"id":"BNpFzI43xhGQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_pol = {}\n","for k in pol_commlist1:\n","  final_pol[k] = []\n","  for v in pol_commlist1[k]:\n","    r = target['page_name'][v]\n","    final_pol[k].append(r)\n","print(final_pol)"],"metadata":{"id":"dZgxDanN4k8g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Node level Classification using Node2Vec\n"],"metadata":{"id":"1WJd1TkXMaSE"}},{"cell_type":"code","source":["g_emb = n2v(G, dimensions=16)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["7f976a49d1eb4c39a7166fd875a7996c","282e127fd8e04816abd97161d2d21c8b","67f5978eb4ed45e8a78cc7f41871d514","b64d320743084cc88d29396b2f6e1ae2","3f42716c528447d7b952bcc3fcf78db8","823030528d784084acb2406e1316607d","2fb1b36a4a8b4a0386f89607aa0d3bed","eeed9c866e9d42e4be13824600b99459","e09fd092954442da86f8ae3233e21905","85a173cd84d74329ac692c4843a4960b","a954d5d761604385b6706332e8e2c534"]},"id":"8GkcCTy94PqU","outputId":"64ca5318-3e81-4db7-df4a-49106cadd982"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Computing transition probabilities:   0%|          | 0/1977 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f976a49d1eb4c39a7166fd875a7996c"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Generating walks (CPU: 1):  90%|█████████ | 9/10 [01:08<00:08,  8.11s/it]"]}]},{"cell_type":"code","source":["WINDOW = 1 # Node2Vec fit window\n","MIN_COUNT = 1 # Node2Vec min. count\n","BATCH_WORDS = 4 # Node2Vec batch words\n","\n","mdl = g_emb.fit(\n","    window=WINDOW,\n","    min_count=MIN_COUNT,\n","    batch_words=BATCH_WORDS\n",")\n","\n","emb_df = (\n","    pd.DataFrame(\n","        [mdl.wv.get_vector(str(n)) for n in G.nodes()],\n","        index = G.nodes\n","    )\n",")\n","\n","\n","\n"],"metadata":{"id":"DW6n_4OS5Zue"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emb_df = emb_df.merge(\n","    target[['id', 'page_name','page_type']].set_index('id'),\n","    left_index = True,\n","    right_index = True\n",")\n","emb_df.head()\n"],"metadata":{"id":"2AyJncMj87iL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(emb_df.page_type.value_counts().head())\n"],"metadata":{"id":"rN3ECceCIh4H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ft_cols = emb_df.drop(columns = ['page_type','page_name']).columns.tolist()\n","target_col = 'page_type'\n","\n","# train test split\n","x = emb_df[ft_cols].values\n","y = emb_df[target_col].values\n","\n","x_train, x_test, y_train, y_test = train_test_split(\n","    x, \n","    y,\n","    test_size = 0.3\n",")\n","\n","# GBC classifier\n","clf = GradientBoostingClassifier()\n","\n","# train the model\n","clf.fit(x_train, y_train)"],"metadata":{"id":"UDr5KiC9It7R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clf_eval(clf, x_test, y_test):\n","    '''\n","    This function will evaluate a sk-learn multi-class classification model based on its\n","    x_test and y_test values\n","    \n","    params:\n","        clf (Model) : The model you wish to evaluate the performance of\n","        x_test (Array) : Result of the train test split\n","        y_test (Array) : Result of the train test split\n","    \n","    returns:\n","        This function will return the following evaluation metrics:\n","            - Accuracy Score\n","            - Matthews Correlation Coefficient\n","            - Classification Report\n","            - Confusion Matrix\n","    \n","    example:\n","        clf_eval(\n","            clf,\n","            x_test,\n","            y_test\n","        )\n","    '''\n","    y_pred = clf.predict(x_test)\n","    y_true = y_test\n","    \n","    y_pred = clf.predict(x_test)\n","    test_acc = accuracy_score(y_test, y_pred)\n","    print(\"Testing Accuracy : \", test_acc)\n","    \n","    print(\"MCC Score : \", matthews_corrcoef(y_true, y_pred))\n","    \n","    print(\"Classification Report : \")\n","    print(classification_report(y_test, clf.predict(x_test)))\n","    \n","    print(confusion_matrix(y_pred,y_test))\n","    \n","clf_eval(\n","    clf,\n","    x_test,\n","    y_test\n",")"],"metadata":{"id":"aZkZsjrOI-DN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["G.nodes()"],"metadata":{"id":"HyulBHSxKceC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_ft = [mdl.wv.get_vector(str('14'))]\n","clf.predict(pred_ft)[0]"],"metadata":{"id":"VNlPX7hzJWrW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#GraphML for Link Prediction"],"metadata":{"id":"Qb2K37l2AuMD"}},{"cell_type":"markdown","source":["## Label Encoding"],"metadata":{"id":"DsZqS3SHWBWj"}},{"cell_type":"code","source":["attr = dict(nx.get_node_attributes(G, \"page_type\"))\n","attr2={'government':0,'politician':1,'tvshow':2,'company':3}\n"],"metadata":{"id":"3SzValaK8Yfd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for x, y in attr.items():\n","    attr[x] = attr2[y]"],"metadata":{"id":"JkhBUz-M94ZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list1=list(attr.values())\n"],"metadata":{"id":"jZc_5JIrAMXu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n","!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n","!pip install torch-geometric\n","!pip install class-resolver"],"metadata":{"id":"QP2S4Ab9A2vx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch_geometric.utils.convert import to_networkx, from_networkx\n","pyg = from_networkx(G_company)\n","pyg.edge_index"],"metadata":{"id":"YxUf0VdpV5uQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attr=dict(nx.get_node_attributes(G_company,\"page_type\"))\n","attr2={'government':0,'politician':1,'tvshow':2,'company':3}\n","for x,y in attr.items():\n","  attr[x]=attr2[y]\n","list3=list(attr.values())\n","print(list3)"],"metadata":{"id":"Hiku0T6jcd-a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data preprocessing"],"metadata":{"id":"_4fuHh4GWHbt"}},{"cell_type":"code","source":["from typing import Callable, Optional\n","\n","import torch\n","\n","from torch_geometric.data import Data, InMemoryDataset\n","\n","\n","class MyDataset(InMemoryDataset):\n","    \n","    def __init__(self, transform: Optional[Callable] = None):\n","        super(MyDataset, self).__init__('.', transform)\n","\n","        edge_index = pyg.edge_index\n","        y = torch.tensor(list3)\n","\n","        x = torch.eye(y.size(0), dtype=torch.float)\n","\n","        # Select a single training node for each community\n","        # (we just use the first one).\n","        train_mask = torch.zeros(y.size(0), dtype=torch.bool)\n","        #for i in range(int(y.max()) + 1):\n","        train_mask[(y == 3).nonzero(as_tuple=False)[0]] = True\n","\n","        data = Data(x=x, edge_index=edge_index, y=y, train_mask=train_mask)\n","\n","        self.data, self.slices = self.collate([data])"],"metadata":{"id":"V0kxCNLzd0Mj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset=MyDataset()"],"metadata":{"id":"UGTBeUZtjX-V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset.data"],"metadata":{"id":"thMLk85rjh94"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## GCN Model"],"metadata":{"id":"u5HfWDaiWK3o"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"h0E0nmsw_2k5"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, confusion_matrix, f1_score,roc_auc_score\n","from sklearn.preprocessing import normalize\n","import torch\n","import torch.nn.functional as F\n","from scipy.sparse.csgraph import shortest_path\n","from torch.nn import BCEWithLogitsLoss, Conv1d, MaxPool1d, ModuleList\n","from itertools import chain\n","import math\n","\n","from torch_geometric.data import InMemoryDataset\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.loader import DataLoader\n","from torch_geometric.nn import MLP, GCNConv, global_sort_pool\n","from torch_geometric.transforms import RandomLinkSplit\n","from torch_geometric.utils import k_hop_subgraph, to_scipy_sparse_matrix\n","\n","class SEALDataset(InMemoryDataset):\n","    def __init__(self, dataset, num_hops, split='train'):\n","        self.data = dataset[0]\n","        self.num_hops = num_hops\n","        super().__init__('.')\n","        index = ['train', 'val', 'test'].index(split)\n","        self.data, self.slices = torch.load(self.processed_paths[index])\n","\n","    @property\n","    def processed_file_names(self):\n","        return ['SEAL_train_data.pt', 'SEAL_val_data.pt', 'SEAL_test_data.pt']\n","\n","    def process(self):\n","        transform = RandomLinkSplit(num_val=0.05, num_test=0.1,\n","                                    is_undirected=True, split_labels=True)\n","        train_data, val_data, test_data = transform(self.data)\n","\n","        self._max_z = 0\n","\n","        # Collect a list of subgraphs for training, validation and testing:\n","        train_pos_data_list = self.extract_enclosing_subgraphs(\n","            train_data.edge_index, train_data.pos_edge_label_index, 1)\n","        train_neg_data_list = self.extract_enclosing_subgraphs(\n","            train_data.edge_index, train_data.neg_edge_label_index, 0)\n","\n","        val_pos_data_list = self.extract_enclosing_subgraphs(\n","            val_data.edge_index, val_data.pos_edge_label_index, 1)\n","        val_neg_data_list = self.extract_enclosing_subgraphs(\n","            val_data.edge_index, val_data.neg_edge_label_index, 0)\n","\n","        test_pos_data_list = self.extract_enclosing_subgraphs(\n","            test_data.edge_index, test_data.pos_edge_label_index, 1)\n","        test_neg_data_list = self.extract_enclosing_subgraphs(\n","            test_data.edge_index, test_data.neg_edge_label_index, 0)\n","\n","        # Convert node labeling to one-hot features.\n","        for data in chain(train_pos_data_list, train_neg_data_list,\n","                          val_pos_data_list, val_neg_data_list,\n","                          test_pos_data_list, test_neg_data_list):\n","            # We solely learn links from structure, dropping any node features:\n","            data.x = F.one_hot(data.z, self._max_z + 1).to(torch.float)\n","\n","        torch.save(self.collate(train_pos_data_list + train_neg_data_list),\n","                   self.processed_paths[0])\n","        torch.save(self.collate(val_pos_data_list + val_neg_data_list),\n","                   self.processed_paths[1])\n","        torch.save(self.collate(test_pos_data_list + test_neg_data_list),\n","                   self.processed_paths[2])\n","\n","    def extract_enclosing_subgraphs(self, edge_index, edge_label_index, y):\n","        data_list = []\n","        for src, dst in edge_label_index.t().tolist():\n","            sub_nodes, sub_edge_index, mapping, _ = k_hop_subgraph(\n","                [src, dst], self.num_hops, edge_index, relabel_nodes=True)\n","            src, dst = mapping.tolist()\n","\n","            # Remove target link from the subgraph.\n","            mask1 = (sub_edge_index[0] != src) | (sub_edge_index[1] != dst)\n","            mask2 = (sub_edge_index[0] != dst) | (sub_edge_index[1] != src)\n","            sub_edge_index = sub_edge_index[:, mask1 & mask2]\n","\n","            # Calculate node labeling.\n","            z = self.drnl_node_labeling(sub_edge_index, src, dst,\n","                                        num_nodes=sub_nodes.size(0))\n","            #print(sub_nodes)\n","            data = Data(x=self.data.x[sub_nodes], z=z,\n","                        edge_index=sub_edge_index, y=y)\n","            data_list.append(data)\n","\n","        return data_list\n","\n","    def drnl_node_labeling(self, edge_index, src, dst, num_nodes=None):\n","        # Double-radius node labeling (DRNL).\n","        src, dst = (dst, src) if src > dst else (src, dst)\n","        adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).tocsr()\n","\n","        idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n","        adj_wo_src = adj[idx, :][:, idx]\n","\n","        idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n","        adj_wo_dst = adj[idx, :][:, idx]\n","\n","        dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True,\n","                                 indices=src)\n","        dist2src = np.insert(dist2src, dst, 0, axis=0)\n","        dist2src = torch.from_numpy(dist2src)\n","\n","        dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True,\n","                                 indices=dst - 1)\n","        dist2dst = np.insert(dist2dst, src, 0, axis=0)\n","        dist2dst = torch.from_numpy(dist2dst)\n","\n","        dist = dist2src + dist2dst\n","        dist_over_2, dist_mod_2 = dist // 2, dist % 2\n","\n","        z = 1 + torch.min(dist2src, dist2dst)\n","        z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)\n","        z[src] = 1.\n","        z[dst] = 1.\n","        z[torch.isnan(z)] = 0.\n","\n","        self._max_z = max(int(z.max()), self._max_z)\n","\n","        return z.to(torch.long)\n","train_dataset = SEALDataset(dataset, num_hops=2, split='train')\n","val_dataset = SEALDataset(dataset, num_hops=2, split='val')\n","test_dataset = SEALDataset(dataset, num_hops=2, split='test')\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32)\n","test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","class DGCNN(torch.nn.Module):\n","    def __init__(self, hidden_channels, num_layers, GNN=GCNConv, k=0.6):\n","        super().__init__()\n","\n","        if k < 1:  # Transform percentile to number.\n","            num_nodes = sorted([data.num_nodes for data in train_dataset])\n","            k = num_nodes[int(math.ceil(k * len(num_nodes))) - 1]\n","            k = max(10, k)\n","        self.k = int(k)\n","\n","        self.convs = ModuleList()\n","        self.convs.append(GNN(train_dataset.num_features, hidden_channels))\n","        for i in range(0, num_layers - 1):\n","            self.convs.append(GNN(hidden_channels, hidden_channels))\n","        self.convs.append(GNN(hidden_channels, 1))\n","\n","        conv1d_channels = [16, 32]\n","        total_latent_dim = hidden_channels * num_layers + 1\n","        conv1d_kws = [total_latent_dim, 5]\n","        self.conv1 = Conv1d(1, conv1d_channels[0], conv1d_kws[0],\n","                            conv1d_kws[0])\n","        self.maxpool1d = MaxPool1d(2, 2)\n","        self.conv2 = Conv1d(conv1d_channels[0], conv1d_channels[1],\n","                            conv1d_kws[1], 1)\n","        dense_dim = int((self.k - 2) / 2 + 1)\n","        dense_dim = (dense_dim - conv1d_kws[1] + 1) * conv1d_channels[1]\n","        self.mlp = MLP([dense_dim, 128, 1], dropout=0.5, batch_norm=False)\n","\n","    def forward(self, x, edge_index, batch):\n","        xs = [x]\n","        for conv in self.convs:\n","            xs += [conv(xs[-1], edge_index).tanh()]\n","        x = torch.cat(xs[1:], dim=-1)\n","\n","        # Global pooling.\n","        x = global_sort_pool(x, batch, self.k)\n","        x = x.unsqueeze(1)  # [num_graphs, 1, k * hidden]\n","        x = self.conv1(x).relu()\n","        x = self.maxpool1d(x)\n","        x = self.conv2(x).relu()\n","        x = x.view(x.size(0), -1)  # [num_graphs, dense_dim]\n","\n","        return self.mlp(x)\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = DGCNN(hidden_channels=32, num_layers=3).to(device)\n","optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n","criterion = BCEWithLogitsLoss()\n","\n","\n","def train():\n","    model.train()\n","\n","    total_loss = 0\n","    for data in train_loader:\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        out = model(data.x, data.edge_index, data.batch)\n","        loss = criterion(out.view(-1), data.y.to(torch.float))\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += float(loss) * data.num_graphs\n","\n","    return total_loss / len(train_dataset)\n","\n","\n","@torch.no_grad()\n","def test(loader):\n","    model.eval()\n","\n","    y_pred, y_true = [], []\n","    for data in loader:\n","        data = data.to(device)\n","        logits = model(data.x, data.edge_index, data.batch)\n","        y_pred.append(logits.view(-1).cpu())\n","        y_true.append(data.y.view(-1).cpu().to(torch.float))\n","\n","    return roc_auc_score(torch.cat(y_true), torch.cat(y_pred))\n","\n","\n","best_val_auc = test_aucGCN = 0\n","for epoch in range(1, 51):\n","    lossGCN = train()\n","    val_auc = test(val_loader)\n","    if val_auc > best_val_auc:\n","        best_val_auc = val_auc\n","        test_aucGCN = test(test_loader)\n","    print(f'Epoch: {epoch:02d}, Loss: {lossGCN:.4f}, Val: {val_auc:.4f}, '\n","          f'Test: {test_aucGCN:.4f}')\n"]},{"cell_type":"markdown","source":["## GAT Model"],"metadata":{"id":"v8-n_pR8WQu4"}},{"cell_type":"code","source":["from torch_geometric.nn.conv.gat_conv import GATConv\n","import torch\n","import torch.nn.functional as F\n","from scipy.sparse.csgraph import shortest_path\n","from torch.nn import BCEWithLogitsLoss, Conv1d, MaxPool1d, ModuleList\n","from itertools import chain\n","import math\n","\n","from torch_geometric.data import InMemoryDataset\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.loader import DataLoader\n","from torch_geometric.nn import MLP, GCNConv, global_sort_pool,SAGEConv,GATConv\n","from torch_geometric.transforms import RandomLinkSplit\n","from torch_geometric.utils import k_hop_subgraph, to_scipy_sparse_matrix\n","\n","class SEALDataset(InMemoryDataset):\n","    def __init__(self, dataset, num_hops, split='train'):\n","        self.data = dataset[0]\n","        self.num_hops = num_hops\n","        super().__init__('.')\n","        index = ['train', 'val', 'test'].index(split)\n","        self.data, self.slices = torch.load(self.processed_paths[index])\n","\n","    @property\n","    def processed_file_names(self):\n","        return ['SEAL_train_data.pt', 'SEAL_val_data.pt', 'SEAL_test_data.pt']\n","\n","    def process(self):\n","        transform = RandomLinkSplit(num_val=0.05, num_test=0.1,\n","                                    is_undirected=True, split_labels=True)\n","        train_data, val_data, test_data = transform(self.data)\n","\n","        self._max_z = 0\n","\n","        # Collect a list of subgraphs for training, validation and testing:\n","        train_pos_data_list = self.extract_enclosing_subgraphs(\n","            train_data.edge_index, train_data.pos_edge_label_index, 1)\n","        train_neg_data_list = self.extract_enclosing_subgraphs(\n","            train_data.edge_index, train_data.neg_edge_label_index, 0)\n","\n","        val_pos_data_list = self.extract_enclosing_subgraphs(\n","            val_data.edge_index, val_data.pos_edge_label_index, 1)\n","        val_neg_data_list = self.extract_enclosing_subgraphs(\n","            val_data.edge_index, val_data.neg_edge_label_index, 0)\n","\n","        test_pos_data_list = self.extract_enclosing_subgraphs(\n","            test_data.edge_index, test_data.pos_edge_label_index, 1)\n","        test_neg_data_list = self.extract_enclosing_subgraphs(\n","            test_data.edge_index, test_data.neg_edge_label_index, 0)\n","\n","        # Convert node labeling to one-hot features.\n","        for data in chain(train_pos_data_list, train_neg_data_list,\n","                          val_pos_data_list, val_neg_data_list,\n","                          test_pos_data_list, test_neg_data_list):\n","            # We solely learn links from structure, dropping any node features:\n","            data.x = F.one_hot(data.z, self._max_z + 1).to(torch.float)\n","\n","        torch.save(self.collate(train_pos_data_list + train_neg_data_list),\n","                   self.processed_paths[0])\n","        torch.save(self.collate(val_pos_data_list + val_neg_data_list),\n","                   self.processed_paths[1])\n","        torch.save(self.collate(test_pos_data_list + test_neg_data_list),\n","                   self.processed_paths[2])\n","\n","    def extract_enclosing_subgraphs(self, edge_index, edge_label_index, y):\n","        data_list = []\n","        for src, dst in edge_label_index.t().tolist():\n","            sub_nodes, sub_edge_index, mapping, _ = k_hop_subgraph(\n","                [src, dst], self.num_hops, edge_index, relabel_nodes=True)\n","            src, dst = mapping.tolist()\n","\n","            # Remove target link from the subgraph.\n","            mask1 = (sub_edge_index[0] != src) | (sub_edge_index[1] != dst)\n","            mask2 = (sub_edge_index[0] != dst) | (sub_edge_index[1] != src)\n","            sub_edge_index = sub_edge_index[:, mask1 & mask2]\n","\n","            # Calculate node labeling.\n","            z = self.drnl_node_labeling(sub_edge_index, src, dst,\n","                                        num_nodes=sub_nodes.size(0))\n","            #print(sub_nodes)\n","            data = Data(x=self.data.x[sub_nodes], z=z,\n","                        edge_index=sub_edge_index, y=y)\n","            data_list.append(data)\n","\n","        return data_list\n","\n","    def drnl_node_labeling(self, edge_index, src, dst, num_nodes=None):\n","        # Double-radius node labeling (DRNL).\n","        src, dst = (dst, src) if src > dst else (src, dst)\n","        adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).tocsr()\n","\n","        idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n","        adj_wo_src = adj[idx, :][:, idx]\n","\n","        idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n","        adj_wo_dst = adj[idx, :][:, idx]\n","\n","        dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True,\n","                                 indices=src)\n","        dist2src = np.insert(dist2src, dst, 0, axis=0)\n","        dist2src = torch.from_numpy(dist2src)\n","\n","        dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True,\n","                                 indices=dst - 1)\n","        dist2dst = np.insert(dist2dst, src, 0, axis=0)\n","        dist2dst = torch.from_numpy(dist2dst)\n","\n","        dist = dist2src + dist2dst\n","        dist_over_2, dist_mod_2 = dist // 2, dist % 2\n","\n","        z = 1 + torch.min(dist2src, dist2dst)\n","        z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)\n","        z[src] = 1.\n","        z[dst] = 1.\n","        z[torch.isnan(z)] = 0.\n","\n","        self._max_z = max(int(z.max()), self._max_z)\n","\n","        return z.to(torch.long)\n","train_dataset = SEALDataset(dataset, num_hops=2, split='train')\n","val_dataset = SEALDataset(dataset, num_hops=2, split='val')\n","test_dataset = SEALDataset(dataset, num_hops=2, split='test')\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32)\n","test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","class DGCNN(torch.nn.Module):\n","    def __init__(self, hidden_channels, num_layers, GNN=GATConv, k=0.6):\n","        super().__init__()\n","\n","        if k < 1:  # Transform percentile to number.\n","            num_nodes = sorted([data.num_nodes for data in train_dataset])\n","            k = num_nodes[int(math.ceil(k * len(num_nodes))) - 1]\n","            k = max(10, k)\n","        self.k = int(k)\n","\n","        self.convs = ModuleList()\n","        self.convs.append(GNN(train_dataset.num_features, hidden_channels))\n","        for i in range(0, num_layers - 1):\n","            self.convs.append(GNN(hidden_channels, hidden_channels))\n","        self.convs.append(GNN(hidden_channels, 1))\n","\n","        conv1d_channels = [16, 32]\n","        total_latent_dim = hidden_channels * num_layers + 1\n","        conv1d_kws = [total_latent_dim, 5]\n","        self.conv1 = Conv1d(1, conv1d_channels[0], conv1d_kws[0],\n","                            conv1d_kws[0])\n","        self.maxpool1d = MaxPool1d(2, 2)\n","        self.conv2 = Conv1d(conv1d_channels[0], conv1d_channels[1],\n","                            conv1d_kws[1], 1)\n","        dense_dim = int((self.k - 2) / 2 + 1)\n","        dense_dim = (dense_dim - conv1d_kws[1] + 1) * conv1d_channels[1]\n","        self.mlp = MLP([dense_dim, 128, 1], dropout=0.5, batch_norm=False)\n","\n","    def forward(self, x, edge_index, batch):\n","        xs = [x]\n","        for conv in self.convs:\n","            xs += [conv(xs[-1], edge_index).tanh()]\n","        x = torch.cat(xs[1:], dim=-1)\n","\n","        # Global pooling.\n","        x = global_sort_pool(x, batch, self.k)\n","        x = x.unsqueeze(1)  # [num_graphs, 1, k * hidden]\n","        x = self.conv1(x).relu()\n","        x = self.maxpool1d(x)\n","        x = self.conv2(x).relu()\n","        x = x.view(x.size(0), -1)  # [num_graphs, dense_dim]\n","\n","        return self.mlp(x)\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = DGCNN(hidden_channels=32, num_layers=3).to(device)\n","optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n","criterion = BCEWithLogitsLoss()\n","\n","\n","def train():\n","    model.train()\n","\n","    total_loss = 0\n","    for data in train_loader:\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        out = model(data.x, data.edge_index, data.batch)\n","        loss = criterion(out.view(-1), data.y.to(torch.float))\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += float(loss) * data.num_graphs\n","\n","    return total_loss / len(train_dataset)\n","\n","\n","@torch.no_grad()\n","def test(loader):\n","    model.eval()\n","\n","    y_pred, y_true = [], []\n","    for data in loader:\n","        data = data.to(device)\n","        logits = model(data.x, data.edge_index, data.batch)\n","        y_pred.append(logits.view(-1).cpu())\n","        y_true.append(data.y.view(-1).cpu().to(torch.float))\n","\n","    return roc_auc_score(torch.cat(y_true), torch.cat(y_pred))\n","\n","\n","best_val_auc = test_aucGAT = 0\n","for epoch in range(1, 51):\n","    lossGAT = train()\n","    val_auc = test(val_loader)\n","    if val_auc > best_val_auc:\n","        best_val_auc = val_auc\n","        test_aucGAT = test(test_loader)\n","    print(f'Epoch: {epoch:02d}, Loss: {lossGAT:.4f}, Val: {val_auc:.4f}, '\n","          f'Test: {test_aucGAT:.4f}')"],"metadata":{"id":"7k4hYSUCCIB_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"AUC_GCN of Company Sub Graph\")\n","print(test_aucGCN)\n","print(\"Loss_GCN of Company Sub Graph \")\n","print(lossGCN)\n","\n","print(\"AUC_GAT of Company Sub Graph\")\n","print(test_aucGAT)\n","print(\"Loss_GAT of Company Sub Graph \")\n","print(lossGAT)"],"metadata":{"id":"XqfU211Sjvx-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["AUC Values for the GAT and GCN model are comparable"],"metadata":{"id":"VI-lnXTWLWAU"}},{"cell_type":"markdown","source":["#Centrality Measures\n"],"metadata":{"id":"wja1XOinlYfR"}},{"cell_type":"code","source":["print(\"Density of the Government Sub Graph\")\n","print(nx.density(G_govt))\n","print(\"Density of the Politician Sub Graph\")\n","print(nx.density(G_politician))\n","print(\"Density of the Tv Shows Sub Graph\")\n","print(nx.density(G_tv))\n","print(\"Density of the Company Sub Graph\")\n","print(nx.density(G_company))\n"],"metadata":{"id":"8xJN6xXZlcBg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The TV shows subgraph is the most dense and the government subgraph (which has the highest nodes) has the least density. But if we observe, company subgraph has the most nodes and yet it has the second lowest density which would imply, node count has minimum impact on density."],"metadata":{"id":"fdlL1pJCWgwu"}},{"cell_type":"code","source":["print(\"Average Clustering of the Government Sub Graph\")\n","print(nx.average_clustering(G_govt))\n","print(\"Average Clustering of the Politician Sub Graph\")\n","print(nx.average_clustering(G_politician))\n","print(\"Average Clustering of the Tv Shows Sub Graph\")\n","print(nx.average_clustering(G_tv))\n","print(\"Average Clustering of the Company Sub Graph\")\n","print(nx.average_clustering(G_company))"],"metadata":{"id":"8jl7x1cSlg3D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Average clustering coefficient is highest for TV shows graph. This says a lot about the domain itself. This implies that majority of people follow mutual shows. "],"metadata":{"id":"R7uGA2f7Xk6E"}},{"cell_type":"code","source":["print(\"Closeness centrality of the Government Sub Graph\")\n","closeness1 = nx.closeness_centrality(G_govt)\n","print(max(sorted(closeness1.items(), key=lambda x: x[1], reverse=True)[0:]))\n","print(\"Closeness Centrality of the Politician Sub Graph\")\n","closeness2 = nx.closeness_centrality(G_politician)\n","print(max(sorted(closeness2.items(), key=lambda x: x[1], reverse=True)[0:]))\n","print(\"Closeness Centrality of the Tv Shows Sub Graph\")\n","closeness3 = nx.closeness_centrality(G_tv)\n","print(max(sorted(closeness3.items(), key=lambda x: x[1], reverse=True)[0:]))\n","print(\"Closeness Centrality of the Company Sub Graph\")\n","closeness4 = nx.closeness_centrality(G_company)\n","print(max(sorted(closeness4.items(), key=lambda x: x[1], reverse=True)[0:]))"],"metadata":{"id":"h37ByJZUlmLn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"00X-Onreiug4"}},{"cell_type":"code","source":["print(\"Page Rank of the Government Sub Graph\")\n","pr1 = nx.pagerank(G_govt, alpha=0.85)\n","print(max(sorted(pr1.items(), key=lambda x: x[1], reverse=True)[0:]))\n","print(target)\n","print(\"Page Rank of the Politician Sub Graph\")\n","pr2 = nx.pagerank(G_politician, alpha=0.85)\n","print(max(sorted(pr2.items(), key=lambda x: x[1], reverse=True)[0:]))\n","print(\"Page Rank of the Tv Shows Sub Graph\")\n","pr3 = nx.pagerank(G_tv, alpha=0.85)\n","print(max(sorted(pr3.items(), key=lambda x: x[1], reverse=True)[0:]))\n","print(\"Page Rank of the Company Sub Graph\")\n","pr4 = nx.pagerank(G_company, alpha=0.85)\n","print(max(sorted(pr4.items(), key=lambda x: x[1], reverse=True)[0:]))"],"metadata":{"id":"3F2yXCFwloLl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["h1, a1 = nx.hits(G_govt)\n","print(\"Hub of Government\")\n","print(max(sorted(h1.items(), key=lambda x: x[1], reverse=True)[0:]))\n","print(\"Authority of Government\")\n","print(max(sorted(a1.items(), key=lambda x: x[1], reverse=True)[0:]))\n","\n","h2, a2 = nx.hits(G_politician)\n","print(\"Hub of Politician\")\n","print(max(sorted(h2.items(), key=lambda x: x[1], reverse=True)[0:]))\n","print(\"Authority of Politician\")\n","print(max(sorted(a2.items(), key=lambda x: x[1], reverse=True)[0:]))\n","\n","h3, a3 = nx.hits(G_tv)\n","print(\"Hub of TV\")\n","print(max(sorted(h3.items(), key=lambda x: x[1], reverse=True)[0:]))\n","print(\"Authority of TV\")\n","print(max(sorted(a3.items(), key=lambda x: x[1], reverse=True)[0:]))\n","\n","h4, a4 = nx.hits(G_company)\n","print(\"Hub of Company\")\n","print(max(sorted(h4.items(), key=lambda x: x[1], reverse=True)[0:]))\n","print(\"Authority of Company\")\n","print(max(sorted(a4.items(), key=lambda x: x[1], reverse=True)[0:]))"],"metadata":{"id":"tQPnEJMWlp3u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wbJFE8ZFHpgX"},"execution_count":null,"outputs":[]}]}